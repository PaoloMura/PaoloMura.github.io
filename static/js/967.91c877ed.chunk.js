"use strict";(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[967],{5967:function(e,s,r){r.r(s);var t=r(2592),i=r(184);s.default=function(){return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("p",{children:"This project was the coursework for my fourth year Cloud Computing and Big Data unit. The challenge was to create a cloud application that could complete an embarassingly parallelisable task and we were given access to an AWS learner lab. I chose to create a render farm, which takes in a Blender project file and renders the animation into a finished MP4 video file."}),(0,i.jsx)("p",{children:"The application itself is constructed using the following AWS services:"}),(0,i.jsxs)("ul",{children:[(0,i.jsx)("li",{children:"Lambda functions"}),(0,i.jsx)("li",{children:"S3 buckets (for storing the input, output and intermediate files)"}),(0,i.jsx)("li",{children:"DynamoDB (for storing a table of job statuses)"}),(0,i.jsx)("li",{children:"EKS cluster (for carrying out the processing of jobs)"}),(0,i.jsx)("li",{children:"SQS queue (for submitting jobs to the cluster)"})]}),(0,i.jsx)(t.Z,{fluid:!0,src:"/images/render-farm/RenderFarm.jpeg"}),(0,i.jsx)("p",{children:"The overall process is as follows:"}),(0,i.jsxs)("ol",{children:[(0,i.jsx)("li",{children:"The client stores the Blender file in S3."}),(0,i.jsx)("li",{children:"The client submits a rendering job to the server."}),(0,i.jsx)("li",{children:"The server splits the job into batches (e.g. frames 1-3, 4-6, ...)."}),(0,i.jsx)("li",{children:"The server sends the batches to the worker cluster."}),(0,i.jsx)("li",{children:"Each worker node processes a batch, rendering their assigned range of frames and storing results to S3."}),(0,i.jsx)("li",{children:"Once all batches are done, one worker node sequences the frames into a single MP4 file."})]}),(0,i.jsx)("p",{children:"On top of the obvious speedup that comes with parallelisation, the application also included scalability and fault tolerance. I used a Kubernetes horizontal pod autoscaler to increase the number of pods in the cluster when % CPU usage increased above a threshold. EKS is itself fault tolerant and I set the SQS queue to make job messages visible again after a timeout from when they were consumed. This allowed jobs to be rescheduled if a node failed to process its batch."}),(0,i.jsx)(t.Z,{fluid:!0,src:"/images/render-farm/Scaling.jpeg"}),(0,i.jsx)("p",{children:"The red line represents % CPU usage. The blue line is the number of pods. The dashed green line is the threshold % CPU usage that determines whether to increase or decrease the number of pods."})]})}}}]);
//# sourceMappingURL=967.91c877ed.chunk.js.map