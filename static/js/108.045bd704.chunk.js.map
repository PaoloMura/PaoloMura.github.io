{"version":3,"file":"static/js/108.045bd704.chunk.js","mappings":"2JAsDA,UAlDA,WACI,OACI,iCACI,yMAGI,SAAC,IAAI,CAACA,KAAK,0DAA0DC,OAAO,SAAQ,yDAE7E,6EAGX,2BACI,kDACA,+EACA,8EAEJ,qJAGA,4QAIA,6KAIA,8LAIA,SAAC,IAAK,CAACC,OAAK,EAACC,IAAI,iCAAiCC,UAAU,WAC5D,iSAKA,6YAKA,2RAMZ,C","sources":["Projects/HPC.js"],"sourcesContent":["import React from 'react'\nimport Image from 'react-bootstrap/Image'\nimport { Link } from '@mui/material'\n\nfunction HPC() {\n    return (\n        <>\n            <p>\n                This project was the coursework for my fourth year High Performance Computing module.\n                We were given some starter code written in C, which was a serial implementation of the \n                <Link href='https://en.wikipedia.org/wiki/Lattice_Boltzmann_methods' target='_blank'>\n                    Lattice Boltzmann fluid dynamics simulation\n                </Link>.\n                The task was to improve the efficiency of this code in three stages:\n            </p>\n            <ol>\n                <li>Serial optimisations</li>\n                <li>Parallelise it across multiple cores using OpenMP</li>\n                <li>Distribute it across multiple nodes using MPI</li>\n            </ol>\n            <p>\n                We also had to analyse the performance of each iteration on Blue Crystal phase 4 (BCp4), the University's supercomputer.\n            </p>\n            <p>\n                Initially, I performed compiler optimisations, loop fusion (collapsing separate functions under a single loop) and optimised complex arithemtic operations (such as divisions).\n                This improved the efficiency of the serial version of the code.\n            </p>\n            <p>\n                Next, I vectorised the code and applied OpenMP parallelism.\n                This allowed the code to run in parallel across all 28 cores of a Blue Crystal node.\n            </p>\n            <p>\n                Finally, I refactored to use MPI instead of OpenMP for parallelism.\n                This allowed the code to be distributed via a halo exchange system, shown in the image below.\n            </p>\n            <Image fluid src='/images/hpc/halo-exchange.jpeg' className='image' />\n            <p>\n                Consider the kth rank (i.e. node), which has n+1 local rows.\n                Its 0th and nth rows are used as \"halo\" regions; local copies of the adjacent rows belonging to the rank above and below.\n                The kth rank only performs processing on the rows under its domain: 1 to n-1.\n            </p>\n            <p>\n                The \"halo exchange\" involves echanging rows between adjacent ranks and is performed after each round of computation.\n                In the diagram above, the kth rank sends a copy of its n-1th row above, while receiving a row from below, which it stores into its 0th row.\n                Then, it sends a copy of its 1st row below, while receiving a row from above, which it stores into its nth row.\n            </p>\n            <p>\n                The results showed a massive speed-up, with the parallel version being 94X faster when compared to the unoptimised code on a grid size of 256x256.\n                The distributed version was 443X faster than the optimised serial code on an input grid size of 1024x1024.\n            </p>\n        </>\n    )\n}\n\nexport default HPC;\n"],"names":["href","target","fluid","src","className"],"sourceRoot":""}